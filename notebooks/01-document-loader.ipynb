{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup\n",
        "Load initial libraries and config to get things up an running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key  = os.environ['OPENAI_API_KEY']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from langchain.docstore.document import Document\n",
        "from typing import Dict, Union\n",
        "import unicodedata\n",
        "\n",
        "def parse_html(file_path: str) -> Document:\n",
        "    with open(file_path, \"r\") as file:\n",
        "        soup = BeautifulSoup(file, \"lxml\")\n",
        "\n",
        "    blog_content = soup.find_all(\"div\", class_=\"blog-item-content\")[0].get_text(separator=u' ', strip=True)\n",
        "    text = unicodedata.normalize(\"NFKC\", blog_content)\n",
        "    \n",
        "    metadata: Dict[str, Union[str, None]] = {\n",
        "        \"source\": file_path,\n",
        "        \"title\": str(soup.title.string),\n",
        "    }\n",
        "\n",
        "    return Document(page_content=text, metadata=metadata)\n",
        "\n",
        "documents = []\n",
        "\n",
        "for file in os.listdir(\"../html\"):\n",
        "    document = parse_html(os.path.join(\"../html\", file))\n",
        "    documents.append(document)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split Documents\n",
        "Split the document into sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1500,\n",
        "    chunk_overlap = 150\n",
        ")\n",
        "\n",
        "sentences = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Store Sentences\n",
        "Store the sentences into a vector database. This will allow us to quickly find similar sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "# Remove old database files if any\n",
        "!rm -rf ../vectors\n",
        "\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=sentences,\n",
        "    embedding=OpenAIEmbeddings(),\n",
        "    persist_directory=\"../vectors/\"\n",
        ")\n",
        "\n",
        "print(vectordb._collection.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test!\n",
        "A quick test to see if the vectors are loaded correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Only run this step if you want to load the vectorized documents from disk\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "vectordb = Chroma(\n",
        "    persist_directory=\"../vectors/\",\n",
        "    embedding_function=OpenAIEmbeddings()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# question = \"Can I give my baby water?\"\n",
        "#uestion = \"What is the best yogurt for my baby?\"\n",
        "question = \"How do I serve a banana?\"\n",
        "response_documents = vectordb.max_marginal_relevance_search(question, k=3)\n",
        "\n",
        "response_documents[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response_documents[1].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response_documents[2].page_content"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
